{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_community.document_loaders import TextLoader, WebBaseLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Loading the data from a text file using TextLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='We are in a generation, where technology has surrounded us from all sides. Our everyday life runs on the use of technology, be it in the form of an alarm clock or a table lamp. Technology has been an important part of our daily lives. Therefore, it is important for the students to be familiar with the term technology. Therefore, we have provided a long speech on technology for students of all age groups. There is also a short speech and a 10 lines speech given in this article.\\n\\nA warm welcome to everyone gathered here today. I am here to deliver a speech on technology which has taken a tremendous role in our day to day life. We all are in a generation where everything is dependent on technology. Letâ€™s understand what technology is through the lens of Science.\\n\\n\\nTechnology comes in the form of tangible and intangible properties by exerting physical and mental force to achieve something that adds value. For example, a mobile phone is tangible, and the network connection used by the phone is intangible. Technology has taken its place as indispensable, wherein it has resulted in economic benefits, better health care, time-saving, and better lifestyle.\\n\\n\\nDue to technology, we have a significant amount of knowledge to improve our lives and solve problems. We can get our work done efficiently and effectively. As long as you know how to access technology, it can be used and proves to benefit people of all ages greatly. Technology is constantly being modified and upgraded every passing year. \\n\\n\\nThe evolution of technology has made it possible to achieve lots in less time. Technology has given tools and machines to be used to solve problems around the world. There has been a complete transformation in the way we do things because of contributions from scientific technology. We can achieve more tasks while saving our time and hence in a better place than our previous generation. \\n\\n\\nRight from the ringing of the morning alarm to switching off the fan, everything runs behind the technology. Even the microphone that I am using is an innovation of technology and thus the list continues. With several inventions of hi-tech products, our daily needs are available on a screen at our fingertips. These innovations and technologies have made our lives a lot easier. Everything can be done at the comfort of your home within a couple of hours or so. These technologies have not only helped us in the digital platform but have also given us innovations in the field of medical, educational, industrial as well as in agricultural sectors. If we go back to the older generations, it would take days to get any things solved, even if there were not many treatments for several diseases. \\n\\n\\nBut today with the innovations of technology, many diseases can be treated and diagnosed within a shorter period of time. The relationship between humans and technology has continued for ages and has given rise to many innovations. It has made it easier for us to handle our daily chores starting from home, office, schools and kitchen needs. It has made available basic necessities and safer living spaces. We can sit at home comfortably and make transactions through the use of online banking. Online shopping, video calling, and attending video lectures on the phone have all been possible due to the invention of the internet. \\n\\n\\nPeople in the past would write letters to communicate with one another, and today due to technology, traditional letters have been replaced by emails and mobile phones. These features are the essential gifts of technology. Everything is just at our fingertips, right from turning on the lights to doing our laundry. The whole world runs on technology and hence, we are solely dependent on it. But everything has its pros and cons. While the benefits of technology are immense, it also comes with some negative effects and possibly irreversible damages to humanity and our planet. \\n\\n\\nWe have become so dependent on technology that we often avoid doing things on our own. It as a result makes us lazy and physically inactive. This has also led to several health issues such as obesity and heart diseases. We prefer booking a cab online rather than walking a few kilometres. Technology has increased screen time, and thus, children are no longer used to playing in the playgrounds but are rather found spending hours on their phones playing video games. This has eroded childrenâ€™s creativity, intelligence, and memory. No doubt, technology is a very essential part of our life, but we should not be totally dependent on it. We should practise being more fit and do regular activities on our own to maintain a healthy lifestyle.\\n\\n\\nThe other aspects that have been badly affected us are that since technology replaced human interference, is unemployment. Social media platforms like Instagram, Facebook, Twitter, etc., were meant to connect people and increase our community circle. Still, it has made people all the more lonely, with cases of depression on the rise amongst the youth. \\n\\nThere are several controversies around the way world leaders have used technology in defence and industrialisation under the banner of development and advancements. The side effects of technology have resulted in pollution, climate change, forest fires, extreme storms, cyclones, impure air, global warming, land area getting reduced and natural resources getting extinct. Itâ€™s time we change our outlook towards selfish technology and bring about responsible technology. Every nation needs to set aside budgets to come up with sustainable technological developments. \\n\\n\\nAs students, we should develop creative problem solving using critical thinking to bring clean technology into our world. As we improve our nation, we must think of our future for a greener and cleaner tomorrow. You would be glad to know that several initiatives have been initiated to bring awareness amongst children and youth to invent cleaner technology. \\n\\n\\nFor example, 15-year-old Vinisha Umashankar invented a solar ironing cart and has been awarded the Earth Shot Prize by the Royal Foundation of the duke and duchess of Cambridge and honoured to speak at the COP26 climate change conference in Glasgow, Scotland. Her invention should be an inspiration to each one of us to pursue clean technology.\\n\\n\\nThe top five technologically advanced countries are Japan, America, Germany, China and South Korea. We Indians will make our mark on this list someday. Technology has a vital role in our lives but lets us be mindful that we control technology and that technology doesnâ€™t control us. Technology is a tool to elevate humanity and is not meant to be a self-destroying mechanism under the pretext of economic development. Lastly, I would like to conclude my speech by saying that technology is a boon for our society but we should use it in a productive way.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a text loader\n",
    "loader = TextLoader(\"speech.txt\")\n",
    "\n",
    "text_document = loader.load()\n",
    "text_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **How to use WebBaseLoader to load data from a web page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://finance.yahoo.com/quote/AAPL/latest-news/'}, page_content='')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "\n",
    "# lets create a web based loader to load news data from yahoofinance\n",
    "news_loader = WebBaseLoader(web_path=(\"https://finance.yahoo.com/quote/AAPL/latest-news/\",),\n",
    "                            bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                                class_=(\"yf-xxbei9\", \"mainContent yf-tnbau3\")\n",
    "                            )))\n",
    "\n",
    "news_text = news_loader.load()\n",
    "news_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **How to read from document like PDF, Word, etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2013; modified using iTextSharp 5.4.1 ©2000-2012 1T3XT BVBA (AGPL-version); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'Microsoft® Word 2013', 'creationdate': '2024-11-14T14:47:55+05:30', 'meeting starting date': '24 Oct. 2024', 'moddate': '2025-01-09T07:26:51-05:00', 'ieee article id': '10830371', 'ieee issue id': '10829993', 'subject': '2024 International Conference on Computing, Sciences and Communications (ICCSC);2024; ; ;10.1109/ICCSC62048.2024.10830371', 'ieee publication id': '10829992', 'title': 'Automated Personalized Mood-Based Song Selector', 'meeting ending date': '25 Oct. 2024', 'source': 'Automated_Personalized_Mood-Based_Song_Selector.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1'}, page_content=\"2024 International Conference on Computing, Sciences and Communications (ICCSC) \\n \\n979-8-3503-5364-8/24/$31.00 ©2024 IEEE \\n \\nAutomated Personalized Mood-Based Song Selector \\n \\nNilay Jain Kanika Sood \\nDepartment of Computer Science Department of Computer Science \\nCalifornia State University, Fullerton California State University, Fullerton  \\nFullerton, USA Fullerton, USA \\nnjain12@csu.fullerton.edu kasood@fullerton.edu \\nAbstract— Music and weather significantly influence \\nindividuals’ moods, playing pivotal roles in daily life. This research \\nexplores integrating music and weather data to enhance the \\ndriving experience by dynamically adjusting the music playlist \\nbased on the driver’s mood and the current weather conditions, \\nconsidering the potential hazards of manual song selection while \\ndriving. The research addresses the challenge of maintaining a \\nconsistent mood during long drives, which can be significantly \\ninfluenced by the music played. We propose a solution that \\ninvolves an AI -integrated application utilizing facial emotion \\nrecognition to gauge the driver’s mood and weather data to infer \\nthe appropriate mood for the music. This mood detection system \\naims to select songs matching the combined mood, enhancing the \\noverall driving experience. We employ machine learning models \\nfor emotion detection from facial expressions and weather data, \\nemploying binary and  multiclass classification techniques. The \\nstudy contributes to personalized music recommendation systems \\nby introducing a novel approach that considers external \\nenvironmental factors, demonstrating the potential for further \\nadvancements in mood-based music recommendation systems.  \\nKeywords—Artificial Neural Network, Face Emotion \\nDetection, Convolutional Neural Network, Image Processing, \\nComputer Vision, AI-Integrated Music Recommendation \\n \\nI.  INTRODUCTION  \\nIn today’s rapidly evolving technological landscape and the \\nincessant rhythm of modern life, the demand for seamless, \\npersonalized experiences has reached new heights. Human \\nemotions play a pivotal role in enriching these experiences, \\nwith musical preferences intricately intertwined with individual \\npersonality traits and emotional states. Emotion, serving as a \\nvital means of communication, manifests through various \\nchannels, including vocal inflections, body language, and facial \\nexpressions. Notably, facial expressions serve as powerful \\nindicators of non -verbal emotions, categorizable into nuanced \\nstates such as neutrality, energy, happiness, and sadness [1]. \\nThe emotive potential of music lies in its diverse properties, \\nwherein distinct song features can evoke varied moods. Factors \\nsuch as danceability, tempo, energy, and instrumentalness exert \\ntangible effects on human emotions, either amplifying or \\nattenuating mood states. Danceability denotes a track’s \\nsuitability for dancing, while tempo defines its pace in beats per \\nminute [2], [3]. Energy, that is quantified on a scale between 0.0 \\nto 1.0, gauges intensity and activity levels, with energetic tracks \\nevoking sensations of rapidity and vigor. Furthermore, \\ninstrumentalness discerns whether a track contains vocals, \\nthereby influencing its emotive resonance.  Beyond facial \\nexpressions, environmental factors, notably  \\n \\nweather conditions, wield considerable influence over human \\nmood states. Sunny weather often engenders feelings of \\nhappiness and vitality, while overcast skies or rainy conditions \\ncan evoke sensations of melancholy or tranquility [4]. Thus, an \\nintricate interplay of musical attributes and environmental \\nstimuli contributes to the multifaceted tapestry of human \\nemotion. In the realm of artificial intelligence, Convolutional \\nNeural Networks (CNNs) eme rge as powerful tools for \\nclassification tasks. Leveraging Conv2D layers with suitable \\nactivation functions, CNNs facilitate both binary and multi -\\nclass classification, enabling the categorization of detected \\nfaces into diverse emotional states like energe tic, happy, \\nneutral, and sad. Furthermore, these models extend their utility \\nbeyond facial expressions, demonstrating proficiency in mood \\ndetection based on weather data [5]. Thus, CNNs stand poised \\nto revolutionize mood recognition across various domains,  \\noffering robust solutions for understanding human emotions in \\nnuanced contexts. \\n \\n   The main contributions of this work are as follows: (1)   we \\nleverage Convolutional Neural Networks (CNNs) for facial \\nemotion recognition and RandomForestClassifier  (RFC) for \\nweather-based mood detection, (2) this research pioneers an AI-\\ndriven approach to enhance driving experiences , (3) b y \\nintegrating these technologies with the Spotify API, the system \\ndynamically adjusts music playlists based on drivers' moods \\nand weather conditions , (4) this  innovative fusion not only \\nensures safer driving by eliminating manual song selection but \\nalso underscores the significance of considering external factors \\nin personalized music recommendation systems  and (5) finally \\ndrawing f rom datasets, AffectNet, FER2013, FER+, and \\nweather data, this study highlights the potential of AI and \\nmachine learning in optimizing user experiences and sets the \\nstage for future advancements in mood -based music selection \\nalgorithms. \\n \\nThis paper is stru ctured as follows. Section II presents the \\nsimilar work done in the past. In the next section we present the \\ntechnique used in this work. Section IV presents our results. In \\nSection V we conclude this work with some future directions. \\n \\nII. RELATED WORK  \\nThe intersection of music recommendation, facial expressions, \\nand live weather conditions presents a compelling  problem \\nwith numerous potential applications. This section provides an \\noverview of relevant research conducted by experts in the \\nfields of music recommendations and face detection.\\n2024 International Conference on Computing, Sciences and Communications (ICCSC) | 979-8-3503-5364-8/24/$31.00 ©2024 IEEE | DOI: 10.1109/ICCSC62048.2024.10830371\\nAuthorized licensed use limited to: California State University Fullerton. Downloaded on March 07,2025 at 09:59:16 UTC from IEEE Xplore.  Restrictions apply.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iTextSharp 5.4.1 ©2000-2012 1T3XT BVBA (AGPL-version); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'Microsoft® Word 2013', 'creationdate': '2024-11-14T14:47:55+05:30', 'meeting starting date': '24 Oct. 2024', 'moddate': '2025-01-09T07:26:51-05:00', 'ieee article id': '10830371', 'ieee issue id': '10829993', 'subject': '2024 International Conference on Computing, Sciences and Communications (ICCSC);2024; ; ;10.1109/ICCSC62048.2024.10830371', 'ieee publication id': '10829992', 'title': 'Automated Personalized Mood-Based Song Selector', 'meeting ending date': '25 Oct. 2024', 'source': 'Automated_Personalized_Mood-Based_Song_Selector.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2'}, page_content='A. Personalized Facial Dataset Creation:  \\nIn the research paper [1], the authors proposed a novel \\napproach on the development of a specialized dataset for the \\nidentification of facial expressions, utilizing the Facial Emotion \\nRecognition using CNN (FERC) model to classify expressions \\nacross five distinct image categories. The dataset comprises \\n10,000 images collected from 154 individuals, ensuring a broad \\nrepresentation of facial expression s. To enhance the dataset’s \\nusability and address potential challenges such as variations in \\nlighting and camera distance, a preprocessing step involving \\nbackground removal is implemented. This meticulous approach \\nto dataset creation and preprocessing aims  to improve the \\naccuracy and reliability of facial expression recognition \\nmodels. \\n \\nB. Facial Emotion Recognition and Music \\nRecommendation System:  \\nIn the realm of facial emotion recognition, the research \\npaper [2], [6] stands out for its innovative approach to emotion \\ndetection using Convolutional Neural Networks (CNNs). The \\nstudy leverages the OAHEGA and FER -2013 datasets, which \\nare rich sources of facial expression images, to train a CNN \\nmodel capable of predicting six primary emotions: anger, fear, \\njoy, neutrality, sadness, and surprise. This work is significant as \\nit demonstrates the potential of deep learning techniques in \\naccurately identifying and classifying human emotions based \\non facial expressions. The use of TensorFlow for constructing \\nand training the CNN model, coupled with the application of \\nthe face recognition Python library, showcases the integration \\nof advanced machine learning algorithms with practical \\napplications in real-time facial recognition systems. \\n \\n• Proposed Solution: The proposed solution in the research \\npaper by Bakariya et al. [1] is a comprehensive system \\ndesigned for real -time facial emotion recognition and \\nmusic recommendation. This system is divided into \\nseveral key components, including Face Detection, Face \\nEmotion Prediction, Music Recommendation, and Face \\nRecognition, with additional functionalities such as a \\nsearch and removal button for previously uploaded faces. \\nThe system’s architecture is designed to operate in real -\\ntime, making it suitable for a wide ra nge of applications \\nwhere immediate emotion recognition is crucial. The use \\nof the Pygame Python package for building the music \\nrecommendation component further underscores the \\nsystem’s versatility and applicability in enhancing user  \\nexperiences through personalized music \\nrecommendations. \\n \\n• DetectMultiScale Function: This function is instrumental \\nin identifying faces in images by detecting objects of  \\nvarious sizes and returning their coordinates. The paper \\nfurther explores the application of the cv2.rectang le \\nmethod to draw rectangles around detected faces, thereby \\nmapping facial features for emotion recognition.  \\nC. Different Networks for Mood Detection  \\nThe paper by Mahadik  [3], titled \"Mood based music \\nrecommendation system,\" utilizes numerous classificat ion \\nmodels to identify correct mood by facial expressions. The \\nimplementation of a mood -based music recommendation \\nsystem presents a novel approach to real -time mood detection \\nand personalized music selection. This system comprises two \\nmain modules: Facial Expression Recognition/Mood Detection \\nand Music Recommendation. The Mood Detection Module is \\ninstrumental in identifying facial expressions and classifying \\nemotions, utilizing advanced techniques such as MobileNet, a \\nlightweight CNN architecture suitable for resource-constrained \\nenvironments. The integration of diverse datasets, including the \\nFER 2013 and MMA Facial Expression Recognition datasets, \\nensures robust training and validation of the emotion \\nclassification model, achieving an accuracy of approxim ately \\n75%.  \\nAdditionally, leveraging the face mesh model provided by \\nMediaPipe [17], our research explores an alternative technique \\nfor facial expression analysis. This model facilitates the plotting \\nof a mesh structure with boundary coordinates delineatin g \\nvarious facial features like eyes, nose, mouth, and cheeks. By \\nutilizing these coordinates, we conducted experiments to \\ngenerate a dataset encompassing coordinates corresponding to \\ndifferent facial expressions, including Calm, Energetic, Happy, \\nand Sad. This approach not only contributes to a more nuanced \\nunderstanding of facial expression dynamics but also offers a \\ncost-effective means of training machine learning models, \\nparticularly in scenarios where large image datasets might pose \\nresource constraints. \\n \\n• Proposed Solution : The Mood Detection Module \\nencompasses two key components: Face Detection and \\nMood Detection. Face Detection involves identifying the \\nlocation of faces within images, employing the \\nFaceDetector class in Java to streamline integration with \\nAndroid applications. Mood Detection further categorizes \\nemotions detected on faces, utilizing MobileNet to \\nclassify emotions into seven distinct categories efficiently. \\nMeanwhile, the Music Recommendation Modu le \\nleverages datasets of songs classified by mood and \\nlanguage, stored and retrieved via Firebase, a cost -\\neffective and easily integrable cloud storage solution. The \\nmp3 versions of songs are manually uploaded to Firebase \\nstorage and linked in the Real -Time database, enabling \\nseamless retrieval and recommendation based on user \\npreferences. \\n \\n• Methodology: The integration of the Facial Expression \\nRecognition and Music Recommendation modules into an    \\nAndroid application involve s several steps. Firstly, the \\ntrained MobileNet model is saved and converted into a \\nTensorFlow Lite (.tflite) file for efficient deployment  on \\nmobile devices. This model is then integrated into the \\nAndroid application alongside the Firebase backend,  \\nAuthorized licensed use limited to: California State University Fullerton. Downloaded on March 07,2025 at 09:59:16 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iTextSharp 5.4.1 ©2000-2012 1T3XT BVBA (AGPL-version); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'Microsoft® Word 2013', 'creationdate': '2024-11-14T14:47:55+05:30', 'meeting starting date': '24 Oct. 2024', 'moddate': '2025-01-09T07:26:51-05:00', 'ieee article id': '10830371', 'ieee issue id': '10829993', 'subject': '2024 International Conference on Computing, Sciences and Communications (ICCSC);2024; ; ;10.1109/ICCSC62048.2024.10830371', 'ieee publication id': '10829992', 'title': 'Automated Personalized Mood-Based Song Selector', 'meeting ending date': '25 Oct. 2024', 'source': 'Automated_Personalized_Mood-Based_Song_Selector.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3'}, page_content=\"The application’s user interface is meticulously designed to \\nprovide a seamless user experience, with intuitive controls for \\nmusic playback and mood -based recommendations. \\nThroughout the development process, rigorous testing is \\nconducted to identify and rectif y any potential bugs, ensuring \\nthe reliability and performance of the final product. \\nAdditionally, user feedback is solicited and incorporated to \\nenhance usability and satisfaction. \\n  \\nIII. METHODOLOGY  \\nPresented by this research, are the detailed steps involved in \\ncreating a comprehensive end -to-end machine learning web \\napplication for detecting mood from human faces and real-time \\nweather and selecting a song based on the detected mood. This \\nincludes preprocessing image data, designing and training \\nconvolutional neural networks for mood detection, integrating \\nweather APIs for real -time weather data retrieval, and  \\nimplementing algorithms for mood -based song \\nrecommendation, all seamlessly orchestrated into a user -\\nfriendly web interface:  \\nA. Mood Detection based on Facial Expressions  \\nThis section discusses the process of detecting mood from \\nhuman faces. \\n \\n1) Dataset Collection and Preprocessing:  The dataset \\ncollection for this research encompasses two distinct \\nmethodologies, each contributing to the creation of a \\ncomprehensive and diversified dataset for facial expression \\nrecognition. The first step involves the acquisition of existing \\ndatasets o f human facial expressions, categorized into four \\ndistinct emotional states: energetic, happy, calm, and sad. \\nThese datasets, sourced from open -sourced repositories, \\nAffectNet, FER2013, and FER+ [5], are instrumental in \\nproviding a broad spectrum of facial  expressions for model \\ntraining [3], [4]. The diversity of these datasets ensures that the \\nmodel is exposed to a wide range of expressions, enhancing its \\nrobustness and generalization capability. Furthermore, careful \\npreprocessing techniques are applied to  standardize and \\noptimize the data for effective model learning. \\n \\nThe second methodology involves the creation of a \\npersonalized dataset through the manual download of images \\nfrom Google, utilizing a Chrome extension designed for bulk \\nimage downloading. Th is method not only expands the \\ndataset’s size but also introduces a level of personalization, \\ncatering to specific research needs or application scenarios. The \\ndataset is divided into training and testing subsets, totaling \\n30,000 images. Fig . 1 shows a sam ple of images of different \\nhuman faces representing different emotions, demonstrating the \\nrichness and variability of the dataset collected. This  \\npersonalized approach allows for a finer granularity in the  \\ntraining data potentially improving the \\n model's performance in recognizing subtle variations in facial \\nexpressions. Additionally, data augmentation techniques such \\nas rotation, scaling, and flipping are applied to further enrich \\nthe dataset and enhance model robustness.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 1. Sample of different human faces. \\n \\nIn the preprocessing phase, images are converted to \\ngrayscale and resized to 48x48 pixels for efficient model \\ntraining. Additionally, a rescaling factor of 1/255 is applied to \\nnormalize pixel values between 0 and 1, enha ncing neural \\nnetwork convergence and stability during training. This \\nnormalization ensures consistent feature scaling, aligning input \\ndata with activation functions for effective learning and faster \\nconvergence. Moreover, techniques such as histogram \\nequalization can be employed to further enhance image contrast \\nand improve model performance. \\n \\n2) CNN Model Architecture for mood detection by face: The \\nmood_detection_model presents a convolutional neural \\nnetwork architecture specifica lly tailored for mood detection \\nfrom facial expressions. It comprises several convolutional \\nlayers with increasing filter sizes, followed by max -pooling \\nlayers for downsampling to capture essential features \\neffectively. Dropout layers are strategically int egrated to \\nmitigate overfitting, ensuring better generalization. \\nFurthermore, the model is concluded with fully connected \\nlayers for classification, providing a comprehensive \\nunderstanding of the input data. The final layer employs \\nSoftMax activation, yiel ding probabilities for four distinct \\nmood categories: energetic, happy, calm, and sad. This \\narchitecture is optimized for accurate mood classification  \\nwhile maintaining computational efficiency, suitable for real -\\ntime applications on resource-constrained devices.\\nAuthorized licensed use limited to: California State University Fullerton. Downloaded on March 07,2025 at 09:59:16 UTC from IEEE Xplore.  Restrictions apply.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iTextSharp 5.4.1 ©2000-2012 1T3XT BVBA (AGPL-version); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'Microsoft® Word 2013', 'creationdate': '2024-11-14T14:47:55+05:30', 'meeting starting date': '24 Oct. 2024', 'moddate': '2025-01-09T07:26:51-05:00', 'ieee article id': '10830371', 'ieee issue id': '10829993', 'subject': '2024 International Conference on Computing, Sciences and Communications (ICCSC);2024; ; ;10.1109/ICCSC62048.2024.10830371', 'ieee publication id': '10829992', 'title': 'Automated Personalized Mood-Based Song Selector', 'meeting ending date': '25 Oct. 2024', 'source': 'Automated_Personalized_Mood-Based_Song_Selector.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4'}, page_content=\"Fig. 2.  CNN architecture for mood detection by face. \\n \\n3) Model Working using OpenCV Real-Time \\nFace Detection:  \\n• Face Recognition:  In the realm of human face detection, \\nvarious methodologies are employed, including the Haar \\nCascade  Classifier,  Histogram  of  Oriented  Gradients \\n(HOG), Feature-Based Methods, and Eigenfaces. For the \\npurpose of this project, the Haar Cascade Classifier stands \\nout due to its superior speed compared to other algorithms. \\nThis  classifier  operates  through  a  series  of  boosted \\nclassifiers  to  identify  objects,  showcasing  its  high \\naccuracy [3]. The underlying principle of this technique is \\nrooted  in  the  edge  or  line  detection  methodology \\nintroduced by Viola and Jones [16] in their 2001 paper \\ntitled “Rapid Object Detection Using a Boosted Cascade \\nof Simple Features.” The 'detectMultiScale' function is \\ninstrumental in this process, as it identi fies objects of \\nvarying sizes within the input image, returning a list of \\nrectangles that represent the detected items, with their \\ncoordinates specified as (x, y, w, h) [5]. Fig. 3 [18] shows \\nhow the Haar  Cascade  detects  the  face  by  extracting \\nfeatures   from   the   image.  The   versatility   of   the \\n'detectMultiScale' function lies in its ability to adapt to \\ndiverse image compositions ,  scaling  efficiently  across \\ndifferent resolutions and aspect ratios, thus enhancing its  \\napplicability in real -world scenarios. Additionally, its \\nrobustness against variations in lighting conditions and \\nfacial orientations further solidifies its effectiveness in \\npractical implementations.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig.3. Haar cascade extracting features from the image. \\n \\nThe flowchart in Fig. 4, depicts a face detection system that \\nutilizes OpenCV to capture a user's face via webcam. A \\nmachine learning model is then loaded to analyze the user's \\nmood based on their facial expressions within the captured \\nframe. If no face is detected, the system restarts the process to \\ncontinuously read frames until a face is found. Once a face is \\nidentified, the system predicts the user's mood using the loaded \\nmodel.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig.4. Flowchart to detect mood from human face \\n \\n4) Model Working using MediaPipe Real-Time \\nFace Detection: \\nApart from training a custom CNN model, we explored the \\noption of using MediaPipe FaceMesh for facial expression \\ndetection [21]. MediaPipe FaceMesh leverages a pre -trained \\nmodel to map 468 facial la ndmarks in real -time, making it a \\ncompelling alternative for expression analysis  [22]. This \\nAuthorized licensed use limited to: California State University Fullerton. Downloaded on March 07,2025 at 09:59:16 UTC from IEEE Xplore.  Restrictions apply.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iTextSharp 5.4.1 ©2000-2012 1T3XT BVBA (AGPL-version); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'Microsoft® Word 2013', 'creationdate': '2024-11-14T14:47:55+05:30', 'meeting starting date': '24 Oct. 2024', 'moddate': '2025-01-09T07:26:51-05:00', 'ieee article id': '10830371', 'ieee issue id': '10829993', 'subject': '2024 International Conference on Computing, Sciences and Communications (ICCSC);2024; ; ;10.1109/ICCSC62048.2024.10830371', 'ieee publication id': '10829992', 'title': 'Automated Personalized Mood-Based Song Selector', 'meeting ending date': '25 Oct. 2024', 'source': 'Automated_Personalized_Mood-Based_Song_Selector.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5'}, page_content=\"approach offers significant advantages over custom CNNs, \\nparticularly in terms of computational efficiency and ease of \\ndeployment. While the custom CNN achieved a commendable \\naccuracy of 94% on our dataset, MediaPipe outperformed it \\nwith a 97% accuracy in detecting facial expressions. The pre -\\ntrained nature of MediaPipe eliminates the need for extensive \\ntraining data, making it faster and more resource-efficient. Our \\nresults indicate that MediaPipe not only simplifies the process \\nof facial expression detection but also enhances accuracy, \\nproving to be a robust solution for real -time applications. This \\ncomparison highlights the effectiveness of MediaPipe as a \\nviable alternative to traditional deep learning models for face \\ndetection tasks. \\n \\n \\nB. Mood Detection based on Weather Conditions: \\n \\nWeather conditions exert a profound influence on the human \\nmood, shaping our emotional states in subtle yet significant \\nways. Sunny days often elevate spirits and energize \\nindividuals, while overcast skies and rainy nights can induce \\nfeelings of sadness and lethargy. Factors such as cloud cover, \\nprecipitation levels, and the distinction between day and night \\nplay crucial roles in this dynamic [8]. These environmental \\ncues interact with individual physiological responses to \\nweather, contributing to the complex interplay between \\nweather and mood. Below steps discusses how the mood can \\nbe determined by the current weather conditions. \\n  \\n1) Dataset Creation:  To construct a dataset for training a \\nmodel capable of predicting mood based on weather conditions, \\nthis research incorporates three primary features: 'is_day' \\n(binary, 0 for night, 1 for day), 'cloud_cover' (percentage, 0 -\\n100%), and 'pre cipitation' (amount, 0 -2). These features are \\npivotal as they significantly influence human mood. The \\n'is_day' feature distinguishes between day and night, \\n'cloud_cover' quantifies the extent of cloud coverage, and \\n'precipitation' measures the amount of rain or snow [8], [9]. By \\nanalyzing thousands of variations of these three features, a \\nmood is associated as the target value, thereby creating a \\ncomprehensive dataset for mood prediction based on weather \\nconditions. \\n \\nTo facilitate the machine learning model 's interpretation of \\nmood data, the target values, which represent moods in the \\ndataset, undergo a transformation process to convert them into \\nnumerical form [10]. This conversion is crucial as most \\nmachine learning algorithms operate on numerical data, \\nnecessitating the translation of categorical mood labels into a \\nnumerical format that the algorithms can interpret. Specifically, \\nthe moods are encoded numerically as follows: 0 for energetic, \\n1 for calm, 2 for happy, and 3 for sad. This numerical encoding \\nfacilitates the model's ability to learn and predict moods based \\non the numerical representation of  moods, thereby enhancing \\nthe model's predictive capabilities and accuracy. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig.5. Dataframe of weather features with target mood \\n \\n2) RandomForestClassifier (RFC) for mood detection by \\nweather:  \\nThe RFC is a machine learning model renowned for its \\nefficacy in classification tasks. Unlike single decision trees, it \\nconstructs a multitude of decision trees during training, \\nutilizing a techn ique known as ensemble learning [15]. \\nThrough this process, the classifier aggregates the predictions \\nof these individual trees, ultimately outputting the class that is \\nthe mode of the classes predicted by the constituent trees. This \\napproach imbues the Ra ndomForestClassifier with resilience \\nagainst overfitting and the capacity to discern complex \\nrelationships within high-dimensional datasets.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig.6.  RFC model to predict mood based on weather features. \\n \\n3) Model Working on Current Weather Conditions:  The \\nprocess of predicting mood based on weather conditions begins \\nwith initializing a Random Forest Classifier (RFC) model, a \\nversatile machine learning technique renowned for its \\naccuracy. Following this, the system utilizes the Open -Meteo \\nweather API to gather an array of current weather features. \\nThese encompass not only basic meteorological data such as \\nAuthorized licensed use limited to: California State University Fullerton. Downloaded on March 07,2025 at 09:59:16 UTC from IEEE Xplore.  Restrictions apply.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iTextSharp 5.4.1 ©2000-2012 1T3XT BVBA (AGPL-version); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'Microsoft® Word 2013', 'creationdate': '2024-11-14T14:47:55+05:30', 'meeting starting date': '24 Oct. 2024', 'moddate': '2025-01-09T07:26:51-05:00', 'ieee article id': '10830371', 'ieee issue id': '10829993', 'subject': '2024 International Conference on Computing, Sciences and Communications (ICCSC);2024; ; ;10.1109/ICCSC62048.2024.10830371', 'ieee publication id': '10829992', 'title': 'Automated Personalized Mood-Based Song Selector', 'meeting ending date': '25 Oct. 2024', 'source': 'Automated_Personalized_Mood-Based_Song_Selector.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6'}, page_content='\\'is_day\\' (a binary indicator distinguishing between night and \\nday), \\'cloud_cover\\' (expressed as a percentage ranging from 0 \\nto 100%), and \\'precipitation\\' (measured in millimeters), but also \\nmore nuanced variables like \\'wind_speed\\' and \\'temperature\\'. \\nOnce these weather data points are collected, they are \\nmeticulously processed and transmitted to the RFC model, \\nwhich has been trained to  classify mood among four distinct \\ncategories: energetic, happy, calm, and sad. The RFC model, \\nan ensemble learning method comprising multiple decision \\ntrees, evaluates the input data to make its predictions. What sets \\nthis system apart is its adaptability; as users\\' mood preferences \\nevolve, the model continuously refines its understanding, \\nensuring that recommendations remain personalized and \\nrelevant. By leveraging the power of machine learning \\nalgorithms, this system continually enhances its proficiency in \\ndiscerning the subtle interplay between mood and weather \\npatterns, thus refining the accuracy of its recommendations \\nwith each interaction. \\n \\nFig. 7 shows the complete process of determining the mood \\nbased on the weather data extracted from the Open -Meteo \\nweather API using the random forest classifier model.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig.7. Workflow to predict mood from weather data. \\n \\n \\nC. Integration of Models with Spotify API for \\nSong Recommendation:  \\nA detailed step -by-step flow is illustrated in Fig. 8, \\ndepicting a meticulously orchestrated sequence of steps aimed \\nat seamlessly integrating multiple components: the predictive \\nmodel, mood detection algorithms based on facial expressions \\nand weather conditions, and the Spotify Developer API, to \\nfacilitate the precise selection of songs corresponding to \\ndetected moods. Initially, the predictive model receives inputs \\nrepresenting the predicted mood from both the facial \\nexpression analysis module and the weather -based mood \\ndetection module. These inputs, deri ved from sophisticated \\nalgorithms trained on diverse datasets, provide nuanced \\ninsights into the emotional states of users. Subsequently, \\nleveraging the extensive capabilities of the Spotify Developer \\nAPI, the model interfaces with the platform to identify  song \\ngenres that best align with the detected moods. This interaction \\ninvolves querying the API for genre recommendations tailored \\nto the specific mood profiles, ensuring a personalized and \\ncontextually relevant song selection process. Upon receiving \\nthe API response containing genre recommendations, the \\nmodel employs a randomized selection mechanism to choose a \\nsong from the recommended genre. This randomization \\nstrategy adds an element of spontaneity and diversity to the \\nsong selection process, enhancing  user engagement and \\nsatisfaction. Finally, the chosen song is seamlessly integrated  \\ninto the application, ready to be played and enjoyed by users,  \\nthereby encapsulating a sophisticated fusion of predictive \\nmodeling, data -driven insights, and API -driven functionality \\nwithin the realm of mood -based song recommendation \\nsystems. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig.8. Steps to integrate models to select song using Spotify Developer \\nAPI \\n \\nIV. RESULTS \\n \\nA. Results from real-time detection: \\nUpon the activation of the  \"Detect Mood\" button on the \\nfront end, the OpenCV library initiates the webcam to capture \\nframes for the purpose of facial expression analysis. This \\nprocess involves capturing a continuous stream of frames for a \\nduration of 10 seconds, with the final fram e being selected for \\nanalysis. This frame is then processed through a Convolutional \\nNeural Network (CNN) model, which specializes in image \\nAuthorized licensed use limited to: California State University Fullerton. Downloaded on March 07,2025 at 09:59:16 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iTextSharp 5.4.1 ©2000-2012 1T3XT BVBA (AGPL-version); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'Microsoft® Word 2013', 'creationdate': '2024-11-14T14:47:55+05:30', 'meeting starting date': '24 Oct. 2024', 'moddate': '2025-01-09T07:26:51-05:00', 'ieee article id': '10830371', 'ieee issue id': '10829993', 'subject': '2024 International Conference on Computing, Sciences and Communications (ICCSC);2024; ; ;10.1109/ICCSC62048.2024.10830371', 'ieee publication id': '10829992', 'title': 'Automated Personalized Mood-Based Song Selector', 'meeting ending date': '25 Oct. 2024', 'source': 'Automated_Personalized_Mood-Based_Song_Selector.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7'}, page_content='classification tasks. The CNN model analyzes the facial \\nexpression within the image and outputs a mood prediction \\nbased on the detected features [11]. \\n \\nConcurrently, the system also determines the user\\'s current \\nlocation and retrieves weather data features utilizing the Open-\\nMeteo weather API. This data, encompassing various \\natmospheric conditions, is then fed into a Ran dom Forest \\nClassifier (RFC) [3]. The RFC, a machine learning algorithm . \\nWith the mood predictions obtained from both the facial \\nexpression analysis and the weather data, the system proceeds \\nto select a song genre that aligns with the overall mood. This \\ndecision-making process is facilitated by the Spotify Developer \\nAPI, which provides access to a vast database of  music genres \\nand playlists. Based on the mood predictions, the API identifies \\na genre that is most likely to match the user\\'s current emotional \\nstate. Subsequently, a random song is selected from the \\nidentified genre and is streamed to the application for playback. \\nThis selection process ensures that the song chosen not only \\nmatches the genre but also aligns with the mood detected by \\nthe system. \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig.9. Emotion detected by face using CNN model and OpenCV \\n \\nAs illustrated in Fig. 9, the mood detected from the last \\nframe captured by the webcam is identified as \"happy\" by the \\nCNN model. In conjunction with the mood inferred from the \\nweather data, which also indicates a happy mood, the system \\nselects the \\'Power-Pop\\' genre. This genre, a subcategory of rock \\nmusic, is characterized by its high tempo and danceable nature. \\nFig. 10 illustrates the process where the selected song, \\ndetermined through the integration of machine learning models \\nand the Spotify API, is redirected to the webpage for playback \\nutilizing the Spotify player [12], [13]. This figure provides a \\nvisual representation of the final step in the recommendation \\nsystem, showcasing how the system\\'s output —a song chosen  \\nbased on the user\\'s detected mood and current weather \\nconditions—is seamlessly integrated into the user\\'s music \\nlistening experience on Spotify. Once the detect mood button \\nis clicked, the backend triggers the webcam using the OpenCV \\nframework in Python. The facial expressions are detected for \\n20 seconds, and the last frame captured is sent to the machine -\\nlearning model for mood detection.  Meanwhile, the Open -\\nMeteo weather API also gets triggered, and the weather details \\ncaptured are sent to the mood detection model. Once the mood \\ngets detected by both the models, an average mood is calculated \\nand is sent to the Spotify API to get the song details like song \\nname, song artist, and song genre. These are retrieved in JSON \\nformat to be displayed on the UI. Additionally, the system \\nrecords user preferences over time, refining its \\nrecommendations to better suit individual tastes and moods, \\nenhancing the overall user experience. This continuous \\nlearning process ensures that the system evolves dynamically, \\nstaying relevant and responsive to the user\\'s changing \\npreferences and moods. Through these mechanisms, the \\napplication not only provides immediate song \\nrecommendations based on mood and weather but also adapts \\nover time to offer increasingly personalized suggestions, \\nfostering long-term user engagement and satisfaction. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig.10. Song played from the power-pop genre due to happy mood. \\n \\nFig. 11 presents the outcomes achieved by the combined \\napplication of machine learning models and the Spotify API . \\nThis Fig. encapsulates the results of the system\\'s mood \\ndetection and genre selection processes, demonstrating the  \\neffectiveness of the integrated approach in accurately \\npredicting the user\\'s musical preferences. \\n \\n \\n \\nAuthorized licensed use limited to: California State University Fullerton. Downloaded on March 07,2025 at 09:59:16 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iTextSharp 5.4.1 ©2000-2012 1T3XT BVBA (AGPL-version); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'Microsoft® Word 2013', 'creationdate': '2024-11-14T14:47:55+05:30', 'meeting starting date': '24 Oct. 2024', 'moddate': '2025-01-09T07:26:51-05:00', 'ieee article id': '10830371', 'ieee issue id': '10829993', 'subject': '2024 International Conference on Computing, Sciences and Communications (ICCSC);2024; ; ;10.1109/ICCSC62048.2024.10830371', 'ieee publication id': '10829992', 'title': 'Automated Personalized Mood-Based Song Selector', 'meeting ending date': '25 Oct. 2024', 'source': 'Automated_Personalized_Mood-Based_Song_Selector.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8'}, page_content=\"Fig.11. Mood detected by both models and genre selected based on mood. \\n \\nB. Accuracy Comparison of pre-trained ML models vs \\nCustom ML model: \\n \\n       The custom CNN model outperforms both VGG16 and \\nVGG19 in mood detection from facial expressions on a dataset \\nof 30,000 images, achieving an accuracy of 94% compared to \\nVGG16's 7 6% and VGG19's 79% [19]. The table in Fig. 12 \\nshows the evaluation metrics of the custom CNN model. While \\nVGG16 and VGG19 have 16 and  19 layers respectively, with \\nlarge model sizes of 138 million and 144 million parameters, \\nthe custom CNN is more compact and faster, making it more \\nefficient in both memory usage and computation. The custom \\nmodel's high accuracy and tailored design make it ideal for this \\ntask, although it requires careful design and lacks transfer \\nlearning benefits. In contrast, VGG16 and VGG19, though \\nrobust and benefiting from pre -training, are computationally \\nexpensive and less effective for this specific task. \\n \\n Precision Recall F1-Score Support \\n1 1.00 0.93 0.96 100 \\n2 1.00 0.98 0.99 100 \\n3 1.00 0.85 0.92 100 \\n4 0.81 1.00 0.89 100 \\nAccuracy   0.94 400 \\nMacro Avg. 0.95 0.94 0.94 400 \\nWeighted Avg. 0.95 0.94 0.94 400 \\n \\nFig.12. Custom CNN model evaluation metrics. \\n \\nVGG16 and VGG19, despite their robust architectures and \\nsuccess in general image classification tasks, did not perform \\nas well in mood detection from facial expressions due to their \\nlarge size and complexity, which may lead to overfitting on a \\nrelatively s maller, specialized dataset like the 30,000 facial \\nimages used here [20]. Their deep architectures with 16 and 19 \\nlayers require extensive computational resources and may \\nstruggle to capture the nuanced features specific to facial \\nexpressions, especially w ithout significant fine -tuning. In \\ncontrast, the custom CNN model, designed specifically for this \\ntask, is likely optimized to extract the most relevant features for \\nmood detection, allowing it to achieve a much higher accuracy \\nof 94%. The custom model’s m ore efficient and targeted \\narchitecture makes it better suited for the specific \\ncharacteristics of the dataset, leading to its superior \\nperformance. \\n \\nV. CONCLUSION AND FUTURE WORK \\n \\nIn this work, we conducted experiments involving the \\ndevelopment of image c lassification models by extending the \\ntraining of established pretrained models (VGG16, VGG19, \\nand MediaPipe), specifically tailored for mood detection from \\ndrivers' facial expressions. We enriched the training process by \\nintroducing supplementary image datasets, aimed at enhancing \\nthe quality and accuracy of the trained models. We quantified \\nperformance using a wide range of metrics: F1 Score, \\nPrecision, and Recall, providing a thorough assessment of their \\nability to categorize emotions into four distinct categories: \\nCalm, Energetic, Happy, and Sad. Additionally, we developed \\nmodels to infer mood from current weather, broadening the \\nscope of mood detection algorithms. \\n \\nHowever, the study has certain limitations. The reliance on pre-\\nexisting datasets may int roduce biases that could limit the \\ngeneralization of the models across diverse populations and \\ndriving conditions. The performance of the models is also \\ninfluenced by the quality and diversity of the supplementary \\nimage datasets used during training. Furth ermore, while the \\nintegration of weather data expands the capability of mood \\ndetection, the models may not fully capture the complex \\ninterplay between various environmental and personal factors \\naffecting mood. \\n \\nDespite these limitations, the research sugge sts promising \\ndirections for future work. Augmenting the current system with \\nfeatures such as personal voice assistance, implementing a \\nfeedback loop to iteratively enhance the performance of the \\nmachine learning models, and developing a more interactive \\nuser interface could further elevate the overall user experience. \\nAs technology evolves and datasets expand, the potential for \\ncreating even more sophisticated and personalized systems \\ntailored to individual preferences and contexts continues to \\ngrow, promi sing an exciting future for AI -driven music \\nrecommendation systems in various domains beyond driving. \\n \\nIn the future, we aim to address the current limitations by \\nincorporating more diverse and representative datasets, adding \\na user feedback module to furt her enhance ML model \\nAuthorized licensed use limited to: California State University Fullerton. Downloaded on March 07,2025 at 09:59:16 UTC from IEEE Xplore.  Restrictions apply.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iTextSharp 5.4.1 ©2000-2012 1T3XT BVBA (AGPL-version); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'Microsoft® Word 2013', 'creationdate': '2024-11-14T14:47:55+05:30', 'meeting starting date': '24 Oct. 2024', 'moddate': '2025-01-09T07:26:51-05:00', 'ieee article id': '10830371', 'ieee issue id': '10829993', 'subject': '2024 International Conference on Computing, Sciences and Communications (ICCSC);2024; ; ;10.1109/ICCSC62048.2024.10830371', 'ieee publication id': '10829992', 'title': 'Automated Personalized Mood-Based Song Selector', 'meeting ending date': '25 Oct. 2024', 'source': 'Automated_Personalized_Mood-Based_Song_Selector.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9'}, page_content='performance, and integrating voice assistance for seamless \\ninteraction and manual control over music selection. These \\nenhancements will contribute to a more robust, user -centered \\nsystem. \\nREFERENCES  \\n[1] Bakariya, B., Singh, A., Singh, H., Raju, P., Rajpoot, R., & \\nMohbey, K. K. (2024). Facial emotion recognition and music \\nrecommendation system using CNN -based deep learning \\ntechniques. Evolving Systems, 15(2), 641-658. \\n[2] Athavle, M. (2021). Music Recommendation Based  on Face \\nEmotion Recognition. Journal of Informatics Electrical and \\nElectronics Engineering (JIEEE), 2, 1-11.  \\n[3] Mahadik, A., Milgir, S., Patel, J., Kavathekar, V., & Jagan, V. \\nB. (2021). Mood based music recommendation system.  \\n[4] Bundo, M., Preisig, M., Merik angas, K., et al. (2023). How \\nambient temperature affects mood: an ecological momentary \\nassessment study in Switzerland.  \\n[5] Dhillon, A., & Verma, G. K. (2020). Convolutional neural \\nnetwork: a review of models, methodologies and applications to \\nobject detection.  \\n[6] Mehendale, N. (2020). Facial emotion recognition using \\nconvolutional neural networks (FERC).  \\n[7] Parkhi, O. M., Vedaldi, A., & Zisserman, A. (2015). Deep Face \\nRecognition.  \\n[8] Barbosa Escobar, F., Velasco, C., Motoki, K., Byrne, D. V., & \\nWang, Q. J. (2021). The temperature of emotions.  \\n[9] Behnke, M., Overbye, H., Pietruch, M., & Kaczmarek, L.  \\nD. (2021). How seasons, weather, and part of day influence \\nbaseline affective valence in laboratory research participants?  \\n[10] Dimolitsas, I., Kantarelis, S., & Fouka, A. (2023). SpotHitPy: A \\nStudy For ML-Based Song Hit Prediction Using Spotify. \\n[11] Thornton, A., Aliyeva, E., & Pande, T. (2021).  \\n[12] Bhowmick, A., Shamkuwar, K., & Jayaseeli, J. D. (2022). Song \\nRecommendation System based on Mood Detection. \\n[13] Medium Article - Music Genre Prediction.  \\n[14] Apao, N. J., Feliscuzo, L. S., Sta. Romana, C. L. C., & Tagaro, \\nJ. A. S. (2020). Multiclass Classification Using Random Forest \\nAlgorithm.  \\n[15] Apao, N. J., Feliscuzo, L. S., Sta. Romana, C. L. C., & Tagaro, \\nJ. A. S. (2020). Multiclass Classification Using Random Forest \\nAlgorithm.  \\n[16] Viola, P., & Jones, M. (2001, December). Rapid object detection \\nusing a boosted cascade of simple features. In Proceedings of \\nthe 2001 IEEE computer society conference on computer vision \\nand pattern recognition. CVPR 2001 (Vol. 1, pp. I-I). Ieee.  \\n[17] B. Thaman, T. Cao and N. Caporusso, \"Face Mask Detection \\nusing MediaPipe Facemesh,\" 2022 45th Jubilee  \\nInternational Convention on Information, Communication and \\nElectronic Technology (MIPRO),  \\nOpatija, Croatia, 2022, pp. 378-382, doi: \\n10.23919/MIPRO55190.2022.9803531. \\n[18]  Shawky, Elham & El -Khoribi, Reda & Shoman, Mahmoud. \\n(2014). Audio -Visual Speech Recognition for People with \\nSpeech Disorders. International Journal of Computer \\nApplications. 96. 975-8887. 10.5120/16770-6337. \\n[19] A. S. Negi, A. Arora, S. Bisht, S. Devliyal, B. V. Kumar and G. \\nKaur, \"Facial Emotion Detection using CNN & VGG16 \\nModel,\" 2024 IEEE 9th International Conference for \\nConvergence in Technology (I2CT), Pune, India, 2024, pp. 1-6, \\ndoi: 10.1109/I2CT61223.2024.10543618. \\n[20]  Vignesh, S., Savithadevi, M., Sridevi, M.  et al. A novel facial \\nemotion recognition model using segmentation VGG -19 \\narchitecture. Int. j. inf. tecnol. 15, 1777–1787 (2023). \\n[21]  Thaman, B., Cao, T., & Capo russo, N. (2022, May). Face mask \\ndetection using mediapipe facemesh. In  2022 45th Jubilee \\nInternational Convention on Information, Communication and \\nElectronic Technology (MIPRO) (pp. 378-382). IEEE. \\n[22] Al-Nuimi, A. M., & Mohammed, G. J. (2021, August). Face \\ndirection estimation based on mediapipe landmarks. In 2021 7th \\nInternational Conference on Contemporary Information \\nTechnology and Mathematics (ICCITM) (pp. 185-190). IEEE. \\n \\n \\nAuthorized licensed use limited to: California State University Fullerton. Downloaded on March 07,2025 at 09:59:16 UTC from IEEE Xplore.  Restrictions apply.')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_loader = PyPDFLoader(\"Automated_Personalized_Mood-Based_Song_Selector.pdf\")\n",
    "\n",
    "pdf_text = pdf_loader.load()\n",
    "pdf_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Convert PDF document to chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2013; modified using iTextSharp 5.4.1 ©2000-2012 1T3XT BVBA (AGPL-version); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'Microsoft® Word 2013', 'creationdate': '2024-11-14T14:47:55+05:30', 'meeting starting date': '24 Oct. 2024', 'moddate': '2025-01-09T07:26:51-05:00', 'ieee article id': '10830371', 'ieee issue id': '10829993', 'subject': '2024 International Conference on Computing, Sciences and Communications (ICCSC);2024; ; ;10.1109/ICCSC62048.2024.10830371', 'ieee publication id': '10829992', 'title': 'Automated Personalized Mood-Based Song Selector', 'meeting ending date': '25 Oct. 2024', 'source': 'Automated_Personalized_Mood-Based_Song_Selector.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1'}, page_content='2024 International Conference on Computing, Sciences and Communications (ICCSC) \\n \\n979-8-3503-5364-8/24/$31.00 ©2024 IEEE \\n \\nAutomated Personalized Mood-Based Song Selector \\n \\nNilay Jain Kanika Sood \\nDepartment of Computer Science Department of Computer Science \\nCalifornia State University, Fullerton California State University, Fullerton  \\nFullerton, USA Fullerton, USA \\nnjain12@csu.fullerton.edu kasood@fullerton.edu \\nAbstract— Music and weather significantly influence \\nindividuals’ moods, playing pivotal roles in daily life. This research \\nexplores integrating music and weather data to enhance the \\ndriving experience by dynamically adjusting the music playlist \\nbased on the driver’s mood and the current weather conditions, \\nconsidering the potential hazards of manual song selection while \\ndriving. The research addresses the challenge of maintaining a \\nconsistent mood during long drives, which can be significantly \\ninfluenced by the music played. We propose a solution that'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iTextSharp 5.4.1 ©2000-2012 1T3XT BVBA (AGPL-version); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'Microsoft® Word 2013', 'creationdate': '2024-11-14T14:47:55+05:30', 'meeting starting date': '24 Oct. 2024', 'moddate': '2025-01-09T07:26:51-05:00', 'ieee article id': '10830371', 'ieee issue id': '10829993', 'subject': '2024 International Conference on Computing, Sciences and Communications (ICCSC);2024; ; ;10.1109/ICCSC62048.2024.10830371', 'ieee publication id': '10829992', 'title': 'Automated Personalized Mood-Based Song Selector', 'meeting ending date': '25 Oct. 2024', 'source': 'Automated_Personalized_Mood-Based_Song_Selector.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1'}, page_content='involves an AI -integrated application utilizing facial emotion \\nrecognition to gauge the driver’s mood and weather data to infer \\nthe appropriate mood for the music. This mood detection system \\naims to select songs matching the combined mood, enhancing the \\noverall driving experience. We employ machine learning models \\nfor emotion detection from facial expressions and weather data, \\nemploying binary and  multiclass classification techniques. The \\nstudy contributes to personalized music recommendation systems \\nby introducing a novel approach that considers external \\nenvironmental factors, demonstrating the potential for further \\nadvancements in mood-based music recommendation systems.  \\nKeywords—Artificial Neural Network, Face Emotion \\nDetection, Convolutional Neural Network, Image Processing, \\nComputer Vision, AI-Integrated Music Recommendation \\n \\nI.  INTRODUCTION  \\nIn today’s rapidly evolving technological landscape and the \\nincessant rhythm of modern life, the demand for seamless,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iTextSharp 5.4.1 ©2000-2012 1T3XT BVBA (AGPL-version); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'Microsoft® Word 2013', 'creationdate': '2024-11-14T14:47:55+05:30', 'meeting starting date': '24 Oct. 2024', 'moddate': '2025-01-09T07:26:51-05:00', 'ieee article id': '10830371', 'ieee issue id': '10829993', 'subject': '2024 International Conference on Computing, Sciences and Communications (ICCSC);2024; ; ;10.1109/ICCSC62048.2024.10830371', 'ieee publication id': '10829992', 'title': 'Automated Personalized Mood-Based Song Selector', 'meeting ending date': '25 Oct. 2024', 'source': 'Automated_Personalized_Mood-Based_Song_Selector.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1'}, page_content='personalized experiences has reached new heights. Human \\nemotions play a pivotal role in enriching these experiences, \\nwith musical preferences intricately intertwined with individual \\npersonality traits and emotional states. Emotion, serving as a \\nvital means of communication, manifests through various \\nchannels, including vocal inflections, body language, and facial \\nexpressions. Notably, facial expressions serve as powerful \\nindicators of non -verbal emotions, categorizable into nuanced \\nstates such as neutrality, energy, happiness, and sadness [1]. \\nThe emotive potential of music lies in its diverse properties, \\nwherein distinct song features can evoke varied moods. Factors \\nsuch as danceability, tempo, energy, and instrumentalness exert \\ntangible effects on human emotions, either amplifying or \\nattenuating mood states. Danceability denotes a track’s \\nsuitability for dancing, while tempo defines its pace in beats per'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iTextSharp 5.4.1 ©2000-2012 1T3XT BVBA (AGPL-version); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'Microsoft® Word 2013', 'creationdate': '2024-11-14T14:47:55+05:30', 'meeting starting date': '24 Oct. 2024', 'moddate': '2025-01-09T07:26:51-05:00', 'ieee article id': '10830371', 'ieee issue id': '10829993', 'subject': '2024 International Conference on Computing, Sciences and Communications (ICCSC);2024; ; ;10.1109/ICCSC62048.2024.10830371', 'ieee publication id': '10829992', 'title': 'Automated Personalized Mood-Based Song Selector', 'meeting ending date': '25 Oct. 2024', 'source': 'Automated_Personalized_Mood-Based_Song_Selector.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1'}, page_content='minute [2], [3]. Energy, that is quantified on a scale between 0.0 \\nto 1.0, gauges intensity and activity levels, with energetic tracks \\nevoking sensations of rapidity and vigor. Furthermore, \\ninstrumentalness discerns whether a track contains vocals, \\nthereby influencing its emotive resonance.  Beyond facial \\nexpressions, environmental factors, notably  \\n \\nweather conditions, wield considerable influence over human \\nmood states. Sunny weather often engenders feelings of \\nhappiness and vitality, while overcast skies or rainy conditions \\ncan evoke sensations of melancholy or tranquility [4]. Thus, an \\nintricate interplay of musical attributes and environmental \\nstimuli contributes to the multifaceted tapestry of human \\nemotion. In the realm of artificial intelligence, Convolutional \\nNeural Networks (CNNs) eme rge as powerful tools for \\nclassification tasks. Leveraging Conv2D layers with suitable \\nactivation functions, CNNs facilitate both binary and multi -'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iTextSharp 5.4.1 ©2000-2012 1T3XT BVBA (AGPL-version); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'Microsoft® Word 2013', 'creationdate': '2024-11-14T14:47:55+05:30', 'meeting starting date': '24 Oct. 2024', 'moddate': '2025-01-09T07:26:51-05:00', 'ieee article id': '10830371', 'ieee issue id': '10829993', 'subject': '2024 International Conference on Computing, Sciences and Communications (ICCSC);2024; ; ;10.1109/ICCSC62048.2024.10830371', 'ieee publication id': '10829992', 'title': 'Automated Personalized Mood-Based Song Selector', 'meeting ending date': '25 Oct. 2024', 'source': 'Automated_Personalized_Mood-Based_Song_Selector.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1'}, page_content=\"class classification, enabling the categorization of detected \\nfaces into diverse emotional states like energe tic, happy, \\nneutral, and sad. Furthermore, these models extend their utility \\nbeyond facial expressions, demonstrating proficiency in mood \\ndetection based on weather data [5]. Thus, CNNs stand poised \\nto revolutionize mood recognition across various domains,  \\noffering robust solutions for understanding human emotions in \\nnuanced contexts. \\n \\n   The main contributions of this work are as follows: (1)   we \\nleverage Convolutional Neural Networks (CNNs) for facial \\nemotion recognition and RandomForestClassifier  (RFC) for \\nweather-based mood detection, (2) this research pioneers an AI-\\ndriven approach to enhance driving experiences , (3) b y \\nintegrating these technologies with the Spotify API, the system \\ndynamically adjusts music playlists based on drivers' moods \\nand weather conditions , (4) this  innovative fusion not only\")]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "text_document_chunks = text_splitter.split_documents(pdf_text)\n",
    "text_document_chunks[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Converting these chunks into a vector and storing the vectors in chromaDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# Define Ollama embedding function\n",
    "ollama_embedding = OllamaEmbeddings(model=\"all-minilm\")\n",
    "\n",
    "# Create a vector store and add documents\n",
    "vector_db = Chroma.from_documents(text_document_chunks, ollama_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of advanced machine learning algorithms with practical \\napplications in real-time facial recognition systems. \\n \\n• Proposed Solution: The proposed solution in the research \\npaper by Bakariya et al. [1] is a comprehensive system \\ndesigned for real -time facial emotion recognition and \\nmusic recommendation. This system is divided into \\nseveral key components, including Face Detection, Face \\nEmotion Prediction, Music Recommendation, and Face \\nRecognition, with additional functionalities such as a \\nsearch and removal button for previously uploaded faces. \\nThe system’s architecture is designed to operate in real -\\ntime, making it suitable for a wide ra nge of applications \\nwhere immediate emotion recognition is crucial. The use \\nof the Pygame Python package for building the music \\nrecommendation component further underscores the \\nsystem’s versatility and applicability in enhancing user  \\nexperiences through personalized music \\nrecommendations.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Facial Emotion Recognition and Music Recommendation System\"\n",
    "result = vector_db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Storing the vectors in the FAISS(Facebook AI Similarity Search)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "faiss_db = FAISS.from_documents(text_document_chunks, ollama_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'involves an AI -integrated application utilizing facial emotion \\nrecognition to gauge the driver’s mood and weather data to infer \\nthe appropriate mood for the music. This mood detection system \\naims to select songs matching the combined mood, enhancing the \\noverall driving experience. We employ machine learning models \\nfor emotion detection from facial expressions and weather data, \\nemploying binary and  multiclass classification techniques. The \\nstudy contributes to personalized music recommendation systems \\nby introducing a novel approach that considers external \\nenvironmental factors, demonstrating the potential for further \\nadvancements in mood-based music recommendation systems.  \\nKeywords—Artificial Neural Network, Face Emotion \\nDetection, Convolutional Neural Network, Image Processing, \\nComputer Vision, AI-Integrated Music Recommendation \\n \\nI.  INTRODUCTION  \\nIn today’s rapidly evolving technological landscape and the \\nincessant rhythm of modern life, the demand for seamless,'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Facial Emotion Recognition and Music Recommendation System\"\n",
    "result = faiss_db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Using the TinyLlama model with prompts and chains and retrieval for generating output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama(model='tinyllama')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# declaring the tinyllama model\n",
    "tinyllm_model = Ollama(model=\"tinyllama\")\n",
    "tinyllm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# designing the chat prompt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# defining the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are a deep research assistant. You answer the questions based on the provided context.\n",
    "Please answer the question in a clear and concise manner.\n",
    "Think before you answer.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the chains\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain  = create_stuff_documents_chain(tinyllm_model, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001B39A9CC2D0>, search_kwargs={})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# declaring the retriever for the FAISS database\n",
    "retriever = faiss_db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the retrieval chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoking the retrieval chain\n",
    "response = retrieval_chain.invoke({\"input\":\"Who are the main authors of the paper?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main authors of the paper mentioned in the context are not specified in the given text, as they are not explicitly stated as well as their affiliations and other details. The authorized license used in IEEE Xplore for accessing the paper is \"IEEE\" (Institute of Electrical and Electronics Engineers).'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
